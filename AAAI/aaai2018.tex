\def\year{2018}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{booktabs}


\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
%   \pdfinfo{
% /Title (2018 Formatting Instructions for Authors Using LaTeX)
% /Author (AAAI Press Staff)}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Neural Knowledge Acquisition via Mutual Attention between Knowledge Graph and Text}
% \author{AAAI Press\\
% Association for the Advancement of Artificial Intelligence\\
% 2275 East Bayshore Road, Suite 160\\
% Palo Alto, California 94303\\
% }
\maketitle
\begin{abstract}

% 最近又有许多工作融合图谱和文本的特征来进行工作并取得了极好的效果。
% 以往的工作大多依赖于图谱内部的结构特征进行图谱填充，以及挖掘和归纳文本的语义信息进行关系抽取，从而获取知识。
% 最近联合学习模型被提出来融合图谱和文本的特征来进行工作，并取得了很好的效果。
% 在归纳总结过去联合模型的基础上，我们提出了一个采用了交替前进的注意力机制的高效联合学习框架。
% 该框架融合的特征可以同时针对图谱推理填充以及关系抽取两个任务.
% 交替前进的注意力机制在特征融合的同时，也在进行对位模型的降噪强化以缓解图谱与文本模型的缺陷。
% 实验结果表明，我们的模型在两个任务上都取得了很好的效果，验证了模型的可靠性。


We propose a general joint representation learning framework for knowledge acquisition (KA) on two tasks, knowledge graph completion (KGC) and relation extraction (RE) from text. In this framework, we learn representations of KGs and text within a unified semantic space and share their parameters. The joint mechanism enables us to take both knowledge graphs (KGs) and text into consideration and perform better KGC and RE. 


Different from existing joint models for KA, our framework is flexible and existing methods for KGC and RE can be easily integrated into our framework. In experiments, we evaluate models on relation extraction and entity link prediction. The results show that the models trained under our joint framework can significantly and consistently improve in comparison with other baselines.


The current knowledge acquisition (KA) mainly bases on two tasks, knowledge graph completion (KGC) and text relation extraction (RE). The previous works usually mine structural features inside knowledge graphs for their completion or learn from textual semantic information to obtain relational knowledge. Some recent joint learning models have been proposed to fuse knowledge graph and text, which outperform single feature methods. Inspired by the pioneering works, we propose an efficient neural joint learning framework with alternating forward attention. The framework embeds knowledge graphs and text at the same time to handle KGC and RE. The interoperable attention mechanism enhances joint models via mutual noise reduction in addition to feature fusion. In experiments, we evaluate models on relation extraction and entity link prediction. The results show that the models trained under our joint framework can significantly and consistently improve in comparison with other baselines for KA.

\end{abstract}



\begin{figure*}[t]
\centering
\includegraphics[width=0.75\textwidth]{figures/framework1.pdf}
\caption{The framework for joint representation learning of KGs and text with alternating forward attention.}
\label{fig:joinglearning}
\end{figure*}

% \noindent 
\section{Introduction}
\label{intro}

% People construct various knowledge graphs (KGs, also known as Knowledge Bases) to organize structural knowledge about the world. A typical knowledge graph (KG) is a multiple relational directed graph with nodes corresponding to entities and edges corresponding to relations between these entities. The facts in KGs are usually recorded as a set of relational triples ($h$, $r$, $t$) with $h$ and $t$ indicating \emph{head} and \emph{tail} entities and $r$ indicating the \emph{relation} between $h$ and $t$, e.g., (\emph{Mark Twain}, \texttt{PlaceOfBirth}, \emph{Florida}). With KGs as mediums, knowledge can be abstracted and play an important role in many applications such as question answering and web search.

People construct various knowledge graphs (KGs, also known as Knowledge Bases) to organize structural knowledge about the world. A typical knowledge graph (KG) is a multiple relational directed graph, whose nodes and edges are corresponding to entities and relations respectively. The facts in KGs are usually recorded as a set of relational triples ($h$, $r$, $t$) with $h$ and $t$ indicating \emph{head} and \emph{tail} entities and $r$ indicating the \emph{relation} between $h$ and $t$, e.g., (\emph{Mark Twain}, \texttt{PlaceOfBirth}, \emph{Florida}). With KGs as mediums, knowledge can be abstracted and play an important role in many applications such as question answering and web search.

Nonetheless, KGs are far from completion. There are typically two tasks to extend KGs, knowledge graph completion (KGC) and text relation extraction (RE). KGC aims to enrich KGs with novel facts based on the network structure of KGs, such as graph-based models \cite{lao2010relational,lao2011random}, tensor-based models \cite{socher2013reasoning,nickel2016holographic} and translating models \cite{bordes2013translating,ji2015knowledge}. RE aims to extract relational facts from the plain text. Many efforts are also devoted to this task, including kernel-based model \cite{zelenko2003kernel}, embedding-based model \cite{gormley2015improved}, and neural models \cite{socher2012semantic,zeng2014relation}. \cite{mintz2009distant} proposes a distant supervision method to align text with the knowledge to generate labeling instances, which is a milestone work for RE and also the pioneering attempt for the combination of KGs and text.

% Although this alignment mechanism is simple, it inspires some works to jointly consider KGs and plain text for KA. For RE, \cite{weston2013connecting} directly sum up two ranking scores of KGs and text for feature fusion. \cite{riedel2013relation} proposes universal schema to jointly embed relation types of KGs and textual patterns to extract information. \cite{vergaEtAl} incorporates neural networks into universal schema to capture pattern semantics generally. \cite{vergamccallum} proposes row-less universal schema to embed entity pairs via functions of their relation types instead of inflexible representations for each pair. For KGC, \cite{wang2014knowledge} simply aligns words in text and entities in KGs. \cite{toutanova2015representing} extracts textual relations from plain text to align with relation embeddings. \cite{zhong2015aligning,xie2016representation,wang2016text} use entity descriptions to enhance entity embeddings. 


Although this alignment mechanism is simple, it inspires some works to consider KGs and text for KA jointly. For RE, \cite{weston2013connecting} sums up two ranking scores of KGs and text for feature fusion. \cite{riedel2013relation,vergamccallum} use universal schema to embed relation types of KGs and textual patterns jointly. For KGC, \cite{wang2014knowledge} aligns words in text and entities in KGs. \cite{toutanova2015representing,xie2016representation} use relevant contents to enhance entity or relation embeddings. 

These joint models perform well with the fused features. However, there are still some challenges need to be solved:

(1) Models adopt the hard joint mechanism, such as \cite{zhong2015aligning}, integrate KGs and their corresponding text into the same embeddings for KGC. For predictions, they also need various source features to get results. The hard joint models trained with well-aligned datasets will show a high ability to capture features. In fact, not all entities and relations in KGs have aligned text, such as detailed descriptions, and their corresponding contents are usually brief and lack of information. Meanwhile, these models either consider only partial information in text (entity descriptions or textual relations), which makes them even more difficult to generalize well.

% (1) These models usually adopt the hard joint mechanism and integrate features from KGs and text together for a single task optimization. They also need various source features for their predictions. 
% Models \cite{zhong2015aligning,xie2016representation,wang2016text} that embed relevant text into KGs for KGC are typical hard joint models. 
% The hard joint models trained with well aligned datasets will show a strong ability to capture features. In fact, not all entities and relations in KGs have aligned text, such as detailed descriptions, and their aligned contents are usually brief and lack of information. Meanwhile, these models either consider only partial information in text (entity descriptions or textual relations), which makes them even more difficult to generalize well.

(2) Models adopt the soft joint mechanism, such as universal schema \cite{riedel2013relation}, is not carried out by direct feature integration. The models of KGs and text are abreast trained within a unified continuous latent space. The features are shared via co-occurrence entity pairs and latent relevance between knowledge relation types and textual patterns. Though this joint mechanism does not require strict data correspondence for training and predictions, general patterns and entity pairs are limited in generalization to all possible contexts. \cite{vergaEtAl,vergamccallum} attempt to capture semantics from patterns instead of original single embeddings for better generalization. However, methods relied on complicated linguistic analysis may bring inevitable errors. In practice, models directly embed semantics from sentences can be more effective and convenient.
 
To address existing issues, we propose a general joint representation learning framework for KA. As shown in Figure \ref{fig:joinglearning}, the framework employs the soft joint mechanism for both KGC and RE, which is in view of the advantages of previous methods. Due to this, the framework is flexible and most existing embedding-based methods for KGC and RE can be easily integrated into the framework. Instead of partial information, we take advantages of both KGs and text via comprehensive alignments with respect to words, entities, and relations at the same time. The comprehensive information will make models work well on the non-strictly aligned data. Moreover, our method applies deep neural networks instead of conventional linguistic analysis to encode the semantics of sentences, which is especially suitable for modeling large-scale Web text. 

In order to alleviate problems caused by noise in datasets, we adopt a novel alternating forward attention. The attention mechanism lets KG and text models use their special information to highlight important features for each other. The noisy data labeled by distant supervision will be distinguished under the guidance of KGs. Meanwhile, textual features can also be fed back for representation learning of KGs. The process of training is pushed forward along with alternating mutual guidance. During this alternating training, models under the framework can enhance their performance together.

We conduct experiments on real-world datasets whose KGs are extracted from Freebase and text is derived from New York Times (NYT) corpus. We evaluate models on both KGC and RE. Experimental results demonstrate our method can effectively perform joint representation learning and obtain more informative knowledge and text representation, which significantly outperforms other baseline methods in KA from either KGs or text.

\section{Related Work}
\label{sec:related}
Our work relates to representation learning of KGs and textual relations, joint learning for KA, and neural networks with attention. We review related works as follows.

\textbf{Representation Learning of KGs.} A variety of approaches have been proposed to encode entities and relations into a continuous low-dimensional space. TransE \cite{bordes2013translating} regards the relation $r$ in each fact ($h$, $r$, $t$) as a translation from $h$ to $t$ within the low-dimensional space. TransE achieves good results and has many improved models, including TransR \cite{lin2015learning}, TransD \cite{ji2015knowledge}, etc. The tensor-based models, such as RESCAL \cite{nickel2011three}, NTN \cite{socher2013reasoning}, DISTMULT \cite{yang2015embedding} and HOLE \cite{nickel2016holographic}, are also effective but trained slowly. In this paper, we incorporate TransE and TransD as representative in our framework to handle representation learning of KGs. 

\textbf{Representation Learning of Textual Relations.} Many works aim to extract relational facts from large-scale text corpora. Mintz \cite{mintz2009distant} proposes distant supervised model. Then MultiR \cite{hoffmann2011knowledge} proposes a multi-instance mechanism and MIML \cite{surdeanu2012multi} proposes a joint model for multiple relations. In recent years, convolutional neural networks (CNN) \cite{zeng2014relation,zeng2015distant}, recurrent neural networks (RNN) \cite{zhang2015relation} and long short-term memory networks (LSTM) \cite{xu2015classifying,miwa2016end} have been proposed to encode sentence semantics, and identify relations between entities in the given sentences. In this paper, we apply CNN to embed textual relations due to its time efficiency.

% These neural models are capable of accurately capturing textual relations without explicit linguistic analysis. 


\textbf{Joint Learning for Knowledge Acquisition.} Some works attempt to combine KGs and text for KA. \cite{weston2013connecting} sums up knowledge and text ranking scores for predictions. \cite{riedel2013relation} proposes universal schema, which uses a soft joint mechanism to share information between relation types of KGs and textual patterns via common entity pairs. These models get an incredible performance in RE. \cite{vergaEtAl} further incorporates neural networks into universal schema to relax constraints imposed by the entity pairs. For KGC, \cite{wang2014knowledge} trains words and entities together to share parameters during their training. \cite{xie2016representation,wang2016text} use neural networks to embed text descriptions into KG embedding spaces. \cite{toutanova2015representing} extracts textual relations using dependency parsing to incorporate text into DISTMULT. 

% These hard joint models fuse the aligned features and enhance entity and relation embeddings. 
% In this paper, we build a soft joint learning framework, which directly mines semantics from the sentences. Meanwhile, we align words, entities, and relations at the same time with non-strictly aligned datasets.

\textbf{Neural Networks with Attention.} In KA, \cite{lin2016neural,luo-EtAl:2017:Long} build a sentence-level attention over multiple instances to reduce weights of noisy instances for RE. \cite{vergamccallum} uses neural networks with attention to merge similar semantic patterns in universal schema. We propose an alternating forward attention in this paper. Our attention combines models and serves as a channel for information sharing. On the other hand, the attention lets models of KGs and text use additional information for mutual noise reduction. 

\section{The Framework}
\label{sec:framework}

In this section we introduce the framework of joint representation learning and alternating forward attention, starting with notations and definitions.

\subsection{Notations and Definitions}

We denote KGs as $G = \{E, R, T\}$, where $E$, $R$ and $T$ indicate the sets of entities, relations and facts respectively. Each fact triple $(h, r, t) \in T$ indicates a relation $r \in R$ between $h \in E$ and $t \in E$.

Accompanying with $G$, we denote the text corpus consisting of sentences as $D$. The vocabulary of $D$ is denoted as $V$. Each sentence in $D$ is a sequence with $n$ words $s = \{w_1, \ldots, w_n\}, w_i \in V$. In each sentence, there are two annotated entity mentions along with a textual relation $r_s \in R$ between them. 

For each entity, relation and word $h, t \in E$, $r \in R$ and $w \in V$, we use the bold face $\mathbf{h}, \mathbf{t}, \mathbf{r}, \mathbf{w} \in \mathbb{R}^{k_w}$ to indicate their low-dimensional vectors respectively, where $k_w$ is the embedding dimension.

% For each entity, relation and word $h, t \in E$, $r \in R$ and $w \in V$, we use bold face $\mathbf{h}, \mathbf{t}, \mathbf{r}, \mathbf{w} \in \mathbb{R}^{k_w}$ to indicate their vectors of dimension $k_w$ respectively, 


\subsection{Overall Framework of Joint Learning}
\label{sec:joint}

% In this work, we propose a joint learning framework for both the KG and text.

In this framework, we aim to jointly learn representations of entities, relations and words within the same continuous space. With denoting all these representations as model parameters $\theta = \{\theta_E, \theta_R, \theta_V\}$, the framework aims to find optimal parameters
\begin{equation}
\hat{\theta} = \mathop{\arg\max}_{\theta} P(G, D | {\theta}),
\end{equation}
where $\theta_E, \theta_R, \theta_V$ are parameters for entities, relations and words respectively. $P(G, D | {\theta})$ is the conditional probability defined over the knowledge graph $G$ and the text corpus $D$ given the parameters $\theta$. The conditional probability can be further decomposed as
\begin{align}
\label{eq:topeq}
P(G,D|{\theta}) = P(G|{\theta_E,\theta_R})P(D|{\theta_V}).
\end{align}

$P(G|\theta_E, \theta_R)$ is responsible to learn representations of both entities and relations from $G$. This formula means to maximize the likelihood of the facts in $G$. $P(D|{\theta_V})$ is responsible to learn representations of sentence words as well as textual relations from the text corpus $D$. This formula means to maximize the likelihood of the sentences and their corresponding textual relations in $D$. 

In summary, we have
\begin{align}
 P(G|{\theta_E,\theta_R}) & = \prod_{(h,r,t) \in G}P((h, r, t)|{\theta_E, \theta_R}), \\
 P(D|{\theta_V}) & = \prod_{s \in D}P((s, r_s)|{\theta_V}),
\end{align}
where $P((h, r, t)|{\theta_E,\theta_R})$ denotes the conditional probability of relational triples $(h, r, t)$ in the knowledge graph $G$ and $P((s, r_s)|{\theta_V})$ denotes the conditional probability of sentences and their corresponding textual relations $(s, r_s)$ in the text corpus $D$.


\subsubsection{Representation Learning of KGs}
\label{sec:kg}

% For representation learning of KGs, we aim to embed entities and relations to capture the semantic correlations between them. 
To learn from relational triples of KGs, we will optimize the conditional probability $P(h|(r, t),{\theta_E, \theta_R})$, $P(t|(h, r),{\theta_E, \theta_R})$, and $P(r|(h, t),{\theta_E, \theta_R})$ instead of $P((h, r, t)|{\theta_E, \theta_R})$

For each entity pair $(h, t)$ in $G$, we define the latent relation embedding $\mathbf{r}_{ht}$ as a translation from $\mathbf{h}$ to $\mathbf{t}$, which can be formalized as
\begin{equation}
\textbf{r}_{ht} = \textbf{t} - \textbf{h}.
\end{equation}
Meanwhile, each triple $(h, r, t) \in T$ has an explicit relation $r$ between $h$ and $t$. Hence, we can define the scoring function for each triple as follows:
\begin{align}
\label{eq:kg_distance}
f_r(h, t) & = b - \lVert \textbf{r}_{ht} - \textbf{r} \rVert  
% \\\nonumber
		% & = b - \lVert (\textbf{t} - \textbf{h}) - \textbf{r}  \rVert.
\end{align}
where $b$ is a bias constant. Based on the above scoring function, the conditional probability can be formalized over all triples in $T$ as follows:
\begin{align}
P(r|(h, t),{\theta_E, \theta_R}) = \frac{\exp(f_r(h, t))}{\sum_{{r'} \in R} \exp(f_{r'}(h, t))}.
\end{align}

$P(t|(h, r), {\theta_E, \theta_R})$ and $P(h|(r, t),{\theta_E, \theta_R})$ are defined in the same way. In fact, this representation objective is consistent with TransE, we have named this model Prob-TransE.

We also adopt TransD to encode relational triples in our framework, 
\begin{align}
&\textbf{r}_{ht} = \textbf{t}_{r} - \textbf{h}_{r}, \\
&\textbf{h}_{r}  = \textbf{M}_{rh}\textbf{h}, \quad \textbf{t}_{r} = \textbf{M}_{rt}\textbf{t}, \\\nonumber
&\textbf{M}_{th} = \textbf{r}_p\textbf{t}_p^{\top}+\textbf{I}^{k_r \times k_w},	\\\nonumber
&\textbf{M}_{rh} = \textbf{r}_p\textbf{h}_p^{\top}+\textbf{I}^{k_r \times k_w},	\nonumber
\end{align}
where $\textbf{r}_p \in \mathbb{R}^{k_r} $ and $\textbf{h}_p, \textbf{t}_p \in \mathbb{R}^{k_w}$ are projection vectors. In order to simplify, $k_r$ and $k_w$ are the same in our framework. We have named this model Prob-TransD.


\subsubsection{Representation Learning of Textual Relations}
\label{sec:relation} 

Given a sentence containing two entities, the sentence usually exposes implicit features of the textual relation between the two entities. We apply CNN for textual relation representation learning.

For the sentence $s$ containing $(h, t)$ with a relation $r_s$, we concatenate the word embedding $\mathbf{w}_i \in \mathbb{R}^{k_w} $ \cite{mikolov2013efficient} and the position embedding $\mathbf{p}_i \in \mathbb{R}^{k_p \times 2} $ \cite{zeng2014relation} to build the input embedding $\mathbf{x}_i \in \mathbb{R}^{k_i} (k_i = k_w + k_p \times 2)$ for each word:
\begin{align}
\mathbf{s} & = \{\mathbf{x}_1,\ldots, \mathbf{x}_n\} \\\nonumber
&=\{[\mathbf{w}_1;\mathbf{p}_1],\ldots, [\mathbf{w}_n;\mathbf{p}_n]\},
\end{align}
where $k_w$ and $k_p$ are dimensions of word embedding and position embedding respectively.

In the convolution layer, we slide a window of size $m$ over the input sequence $\mathbf{s}$. For each move, we can get a hidden layer vector as:
\begin{align}
\mathbf{\hat{x}}_i &= \big[ \mathbf{x}_{i - \frac{m-1}{2}}; \ldots ; \mathbf{x}_i; \ldots ;\mathbf{x}_{i + \frac{m-1}{2}} \big],\\
\mathbf{h}_i &= \tanh(\mathbf{W}\mathbf{\hat{x}}_i + \mathbf{b}),
\end{align}
where $\mathbf{W} \in \mathbb{R}^{k_c \times mk_i}$ is the convolution kernel, $\mathbf{b} \in \mathbb{R}^{k_c}$ is a bias vector, $k_c$ is the dimension of hidden layer vectors.

In the pooling layer, a max-pooling operation over the hidden layer vectors ${\mathbf{h}_1, \ldots , \mathbf{h}_n}$ is applied to get the final output embedding $\mathbf{y}$ as follows:
\begin{equation}
[\mathbf{y}]_{j} = \max \{[\mathbf{h}_{1}]_{j}, \ldots, [\mathbf{h}_{n}]_{j} \},
\end{equation}
where $[\mathbf{y}]_{j}$ and $[\mathbf{h}_{i}]_{j}$ are the $j$-th value of the output embedding $\mathbf{y}$ and  the hidden vector $\mathbf{h}_i$ respectively. Our method will further get the scoring function,
\begin{equation}
\mathbf{o} = \mathbf{M}\mathbf{y},
\label{eq:cnn_distance}
\end{equation}
where $\mathbf{M} \in \mathbb{R}^{\|R\| \times k_c} $ is the representation matrix to calculate the relation scorings. Then we define the conditional probability $P((s, r_s)|{\theta_V})$ as follows:
\begin{equation}
P((s, r_s)|{\theta_V}) = \frac{\exp(\mathbf{o}_{r_s})}{\sum_{r \in R} \exp(\mathbf{o}_{r})}.
\label{eq:cnn_distance1}
\end{equation}

\subsection{Alternating Forward Attention}

Our alternating forward attention consists of two parts, the knowledge-based attention to guide the text model and the semantics-based attention to guide the knowledge model. Both parts cooperate with each other during the training.

\subsubsection{Knowledge-based Attention}

For each $(h, r_s, t) \in T$, there may be several sentences $\pi_{r_s}=\{s_1,\ldots,s_m\}$ contain $(h, t)$ and imply the relation $r_s$. These sentences' output embeddings are $\{\mathbf{y}_1, \ldots , \mathbf{y}_m\}$, where $m$ is the number of sentences. Sentences labeled by the distant supervision algorithm contain some vague and wrong semantic components. Hence, we argue that some sentences contribute more to the final textual relation representation. 

Under the joint learning framework, additional logical knowledge information can be used to enhance sentence embedding. We use latent relation embedding $\mathbf{r}_{ht} \in \mathbb{R}^{k_w} $ as knowledge-based attention over sentences to highlight important sentences and reduce noisy components:
\begin{align}
\label{eq:att_k}
\mathbf{e}_j & = \tanh(\mathbf{W}_s\mathbf{y}_j+\mathbf{b}_s), \\\nonumber
a_j & =\frac{\exp(\mathbf{r}_{ht}\cdot\mathbf{e}_j)}{\sum_{k = 1}^{m} \exp(\mathbf{r}_{ht}\cdot\mathbf{e}_k)}, \\\nonumber
\mathbf{r}_s & = \sum_{j = 1}^{m} a_j\mathbf{y}_j,
\end{align}
where $\mathbf{W}_s \in \mathbb{R}^{k_w \times k_c}$ is the weight matrix and $\mathbf{b}_s \in \mathbb{R}^{k_w}$ is the bias vector. $a_j$ is the weight for the $j$th sentence output $\mathbf{y}_j$. We take a weighted sum of sentence output embeddings for the global textual relation representation $\mathbf{r}_s$. Then, we formalize $P((\pi_{r_s}, r_s)|{\theta_V})$ instead of $\prod_{j=1}^{m}P(s_j, r_s|{\theta_V})$ as follows,
\begin{align}
P((\pi_{r_s}, r_s)|{\theta_V}) &= \frac{\exp(\mathbf{o}_{r_s})}{\sum_{r \in R} \exp(\mathbf{o}_{r})},	\\\nonumber
\mathbf{o} &= \mathbf{M}\mathbf{r}_s.
\end{align}

\subsubsection{Semantics-based Attention}

For each relation $r \in R$, there are several entity pairs $\pi_r = \{(h_1, t_1), \ldots , (h_n, t_n)\}$ can form fact triples in $T$ with the relation $r$. The latent relation embeddings of these pairs are $\{\mathbf{r}_{h_1t_1}, \ldots , \mathbf{r}_{h_nt_n}\}$, where $n$ is the number of entity pairs. In knowledge graph representation models, we hope that all the latent relation embeddings between entity pairs are close to the explicit relation. 

Because of complicated related situations between entities and errors came from initial construction of KGs, the training process is difficult to let explicit relations exactly fit all latent relations. In order to make knowledge graph representation models more effective, we attempt to use semantic information extracted from text models to help explicit relations fit most reasonable entity pairs as follows:
\begin{align}
&\mathbf{e}_r  = \tanh(\mathbf{W}_s\mathbf{M}_r+\mathbf{b}_s), \\\nonumber
&b_j  =\frac{\exp(\mathbf{e}_{r}\cdot\mathbf{r}_{h_jt_j})}{\sum_{k = 1}^{n} \exp(\mathbf{e}_{r}\cdot\mathbf{r}_{h_kt_k})}, \\\nonumber
&\mathbf{r}_{k}  = \sum_{j = 1}^{n} b_j\mathbf{r}_{h_jt_j},
\end{align}
where $\mathbf{W}_s$ and $\mathbf{b}_s$ is the same weight matrix and bias vector used in Eq. (\ref{eq:att_k}) to map vectors in the neural space to the entity and relation space. $\mathbf{M}_r$ is the text representation embedding for the relation $r$ used in Eq. (\ref{eq:cnn_distance}), which contains textual relation semantics. $b_j$ is the weight for the $j$th latent relation embedding $\mathbf{r}_{h_jt_j}$.

We merge these entity pairs and formalize the conditional probability $P(r|\pi_r,{\theta_E, \theta_R})$ instead of original formalization $\prod_{j=1}^{n}P(r|(h_j, t_j),{\theta_E, \theta_R})$ as follows,
\begin{align}
P(r|\pi_r,{\theta_E, \theta_R}) &= \frac{\exp(f_r(\pi_r))}{\sum_{{r'} \in R} \exp(f_{r'}(\pi_r))},\\\nonumber
f_r(\pi_r) &= b - \lVert \textbf{r}_{k} - \textbf{r} \rVert.
\end{align}
% where $\mathbf{r}_{k}$ is the weighted sum of latent relation embeddings. 








%  Sentences labeled by the distant supervision algorithm contain some vague and wrong semantic components. Hence, we argue that some sentences contribute more to the final textual relation representation. Under the joint learning framework, additional logical knowledge information can be used to enhance sentence embedding. We use  $\mathbf{r}_{ht} \in \mathbb{R}^{k_w} $ as knowledge-based attention over sentences to highlight important sentences and reduce noisy components:


\subsection{Initialization and Implementation Details}
\label{sec:detail}
Here we introduce the learning and optimization details for our joint model. We define the optimization function as the log-likelihood of the objective function in Eq. (\ref{eq:topeq}),
\begin{align}
\mathcal{L}_{\theta}(G, D) & = \log P(G,D|{\theta}) + \lambda \lVert \theta \rVert_2 \\\nonumber
 & = \log P(G|{\theta_E, \theta_R}) + \log P(D|{\theta_V}) \\\nonumber
 & + \lambda \lVert \theta \rVert_2
\end{align}
where $\lambda$ is a harmonic factor, and $\lVert \theta \rVert_2$ is the regularizer defined as $L_2$ distance. Both Prob-TransE, Prob-TransD and CNN are optimized simultaneously using stochastic gradient descent (SGD). 

% The word embeddings used for CNN are pre-trained from plain text by Skip-Gram \cite{mikolov2013efficient}. In practice, we will optimize knowledge and text models in parallel. 


\section{Experiments}
\label{sec:experiments}

We conduct experiments on entity link prediction for KGC, and textual relation extraction from sentences for RE. We evaluate the performance of our joint model with various baselines. 


\subsection{Experiment Settings}



\subsubsection{Datasets}

The datasets used for experiments contain two parts, knowledge graphs and text, whose details are as follows.

\textbf{Knowledge Graph.} We select Freebase \cite{bollacker2008freebase} as the KG for joint learning. Freebase is a widely-used large-scale KG. In this paper, we adopt datasets extracted from Freebase, FB15K and FB60K in our experiments. FB15K has been used as the benchmark for KGC. FB60K is extended from the dataset developed by \cite{riedel2010modeling}, which has been used as the benchmark for relation extraction. We list the statistics of FB15K and FB60K in Table \ref{tab:statistics-of-FB15K}, including the number of entities, relations, and facts.

\begin{table}[htb]
\centering
\small
\begin{tabular}{c|ccc}
\toprule
\textbf{Dataset} & \textbf{Relation} & \textbf{Entity} & \textbf{Fact} \\ 
\midrule
FB15K   & 1,345           & 14,951            & 592,213 \\ 
FB60K   & 1,324           & 69,512            & 335,350 \\ 
\bottomrule
\end{tabular}
\caption{The statistics of FB15K and FB60K}
\label{tab:statistics-of-FB15K}
\end{table}

\textbf{Text Corpus.} We select sentences from the articles of New York Times. We extract $194,385$ sentences containing both head and tail entities in FB15K, and annotate with the corresponding relations in triples. The sentences are labeled with $47,103$ FB15K triples, including $699$ relations and $6053$ entities. We name the corpus as NYT-FB15K. The sentences for FB60K come from the dataset used in \cite{riedel2010modeling}, containing $570,088$ sentences, $63,696$ entities, $56$ relations and $293,175$ facts. We name the corpus as NYT-FB60K.
 
Following the previous usages of these datasets, FB15K and NYT-FB15K are used as the benchmark for KGC, FB60K and NYT-FB60K are used as the benchmark for RE in our experiments.


\subsubsection{KG-Text Alignment}
\label{sec:alignment}

Since entities and relations are not explicitly labeled in text, we have to identify entities and relations in text to support joint representation learning. The process is realized by entity-text alignment and relation-text alignment.

\textbf{Entity-Text Alignment.} Many entities are mentioned in text. Due to the complex polysemy of entity mentions (e.g., an entity name Washington in a sentence could indicate either a person or a location), it is non-trivial to build entity-text alignment. In this paper, we simply use the anchor text annotated in articles to build the alignment between entities in $E$ and entity mentions in $V$. \footnote{In our framework, we set that those entities in $\theta_E$ and their corresponding mentions in $\theta_V$ share the same representations.}

\textbf{Relation-Text Alignment.} Inspired by the idea of distant supervision \cite{min2013distant}, for a relation $r \in R$, we collect all entity pairs $Pair_{r} = \{(h, t) | (h, r, t) \in T \}$ connected by $r$. Afterwards, for each entity pair in $Pair_{r}$, we extract all sentences from $D$ that contain the both entities, and regard them as the positive instances of the relation $r$. 

\subsubsection{Parameter Settings}

In our joint model, we select the learning rate $\alpha_k$ for $P(G|{\theta_E,\theta_R})$ among $\{0.1, 0.01, 0.001\}$, and learning rate $\alpha_t$ for $P(D|{\theta_V})$ among $\{0.1, 0.01, 0.001\}$. The sliding window size $m$ is among $\{3,5,7\}$. For other parameters, since they have limited effect on the results, we simply follow the settings used in \cite{zeng2014relation,lin2016neural} so that we can fairly compare joint learning results with these baselines. To compare with previous works, the dimension $k_w$ is $50$ for RE and $100$ for KGC. In Table \ref{parameters} we show all parameters used in the experiments.


\begin{table}[htb]
\centering
\scalebox{0.9}{
\begin{tabular}{c|c}
\toprule
\multicolumn{1}{l|}{Harmonic Factor $\lambda$}                & 0.0001 \\
\multicolumn{1}{l|}{Knowledge Learning Rate $\alpha_k$}        & 0.001 \\
\multicolumn{1}{l|}{Text Learning Rate $\alpha_t$}             & 0.01  \\
\multicolumn{1}{l|}{Hidden Layer Dimension $k_c$}        & 230   \\
\multicolumn{1}{l|}{Word/Entity/Relation Dimension $k_w$} & 50    \\
\multicolumn{1}{l|}{Position Dimension $k_p$}            & 5     \\
\multicolumn{1}{l|}{Window Size $m$}    & 3     \\
\multicolumn{1}{l|}{Dropout Probability $p$}            & 0.5  \\
\bottomrule
\end{tabular}}
\caption{Parameter settings}
\label{parameters}
\end{table}


\subsection{Results of Relation Extraction}

Most models take distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers. We want to investigate the effectiveness of our joint model for improving CNN models via this task.

\subsubsection{Evaluation Results}

We follow \cite{weston2013connecting} to conduct evaluation. The evaluation constructs candidate triples by combining entity pairs in the testing set with various relations, asks systems to rank these triples according to the corresponding sentences of entity pairs, and by regarding the triples in KGs as correct and others as incorrect, evaluates systems with precision-recall curves.

The evaluation results on NYT-FB60K test set are shown in Figure \ref{fig:jointcnn}, where ``JointD+KATT'' and ``JointE+KATT'' indicate the CNN model with knowledge-based attention learned jointly with Prob-TransD and Prob-TransE respectively, and ``CNN+ONE'' indicates the CNN model with the at-least-one mechanism \cite{zeng2015distant}. ``CNN+ATT'' indicates the CNN model with sentence-level attention \cite{lin2016neural}, which is the state-of-the-art method for RE. We also compare these neural models with feature-based methods, including Mintz \cite{mintz2009distant}, MultiR \cite{hoffmann2011knowledge}, MIML \cite{surdeanu2012multi} and Sm2r \cite{weston2013connecting}. The results are also shown in Figure \ref{fig:jointcnn}. From the results, we observe that: 

(1) Compared with feature-based methods in Figure \ref{fig:jointcnn}, the joint models significantly outperform all these methods over the entire range of recall. The joint models have $10\%$ to $20\%$ improvements when the recall is larger than $0.15$. When the recall is smaller than $0.15$, the joint models also preserve stable and competitive precisions. 

(2) Besides JointD+KATT and JointE+KATT, CNN+ATT and CNN+ONE also have more than $10\%$ improvements when the recall is larger than $0.15$. All these demonstrate that deep neural models which are not restricted to the feature engineering are robust and effective. 

(3) Though the results of feature-based methods drop much more faster, they have reasonable precisions among the recommendations with the highest scores. It shows that human-designed features are very effective but still limited in some fields. There need powerful models to grasp these features. In the future, it is a very meaningful attempt to add these features to our joint learning framework as extra guidance.

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{res.eps}
\caption{Aggregate precision/recall curves of different RE models.}
\label{fig:jointcnn}
\end{figure} 


\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{c|ccc|ccc} 
\toprule
\textbf{P@N(\%)} & \multicolumn{3}{c|}{100}	& \multicolumn{3}{c}{300}	\\
\midrule
\textbf{Method} & \multicolumn{1}{c}{ONE} & \multicolumn{1}{c}{ATT} & KATT & \multicolumn{1}{c}{ONE} & \multicolumn{1}{c}{ATT} & KATT \\ 
\midrule
CNN+	& 67.3	& \textbf{76.2}	& -     & 58.1	& 59.8 	& -    \\ 
JointE+  & 67.5 	& 74.1	& 75.8  & 63.0 	& 63.2 	& 68.0  \\ 
JointD+  & \textbf{68.5}    & 74.6  & \textbf{80.6}  & \textbf{67.0} & \textbf{67.3} & \textbf{68.7}  \\ 
\bottomrule
\toprule
% \midrule

\textbf{P@N(\%)} & \multicolumn{3}{c|}{500}	& \multicolumn{3}{c}{Mean} \\ 
\midrule
\textbf{Method}  & \multicolumn{1}{c}{ONE} & \multicolumn{1}{c}{ATT} & KATT & \multicolumn{1}{c}{ONE} & \multicolumn{1}{c}{ATT} & KATT \\ 
\midrule
CNN+	& 43.7	& 48.5			& -     & 56.4 	& 61.5 	& -     \\ 
JointE+  & 57.3 	& 59.3 	& 63.0  & 62.6 	& 65.5  & 68.9  \\ 
JointD+  & \textbf{58.6}    & \textbf{61.1}  & \textbf{63.7}  & \textbf{64.8}   & \textbf{67.7}    & \textbf{71.0}  \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on P@N with different model combination (\%).}
\label{t:relationExt}
\end{table}



\subsubsection{Effect of Joint Learning and Knowledge-based Attention}

For RE, we usually pay more attention to the recommendations with the highest confidence scores. To detailly compare the results before and after implementing joint learning, we empirically compare different methods through the prediction accuracy over of recommendations with the highest scores. We select the CNN model used in \cite{zeng2014relation} as the sentence encoders. These encoders are combined with different kinds of multi-instance learning methods, including the at-least-one mechanism (ONE), sentence-level attention (ATT) and our knowledge-based attention (KATT). ``JointD'' and ``JointE'' indicate the CNN models learned jointly with Prob-TransD and Prob-TransE respectively, and ``CNN'' indicates the CNN model learned without joint learning. The results are shown in Table \ref{t:relationExt}, including P@100, P@300, P@500 and the mean of them. From the results, we observe that:





\begin{table*}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{c|cccc|cccc|c}

\toprule

\multirow{2}{*}{\textbf{Method}}            	& \multicolumn{4}{c|}{\textbf{Predicting Head}} & \multicolumn{4}{c|}{\textbf{Predicting Tail}} & \multicolumn{1}{c}{\textbf{Overall}} \\ 

 & 1-to-1     & 1-to-N    & N-to-1    & N-to-N    & 1-to-1     & 1-to-N    & N-to-1    & N-to-N  & Triple Avg. \\ 
\midrule
SE \cite{bordes2011learning}&35.6 &62.6 &17.2 &37.5 &34.9 &14.6 &68.3 &41.3 &39.8  \\ 
SME \cite{bordes2012joint}&35.1 &69.6 &19.9 &40.3 &32.7 &14.9 &76.0 &43.3 &41.3  \\ 
TransE \cite{bordes2013translating}            & 43.7       & 65.7      & 18.2      & 47.2      & 43.7       & 19.7      & 66.7      & 50.0    & 47.1  \\ 
TransH \cite{wang2014transh}    & 66.8       & 87.6      & 30.2      & 64.5      & 65.5       & 39.8      &83.3      & 67.2    & 64.4  \\ 
TransR \cite{lin2015learning}   & 78.8       & 89.2      & 38.1      & 66.9      & 79.2       & 38.4      &90.4      & 72.1    & 68.7  \\ 

TransD \cite{ji2015knowledge} &81.2  &94.8  &47.1 &79.3  &81.6 &53.9 &93.7 &82.5  &78.9  \\

\midrule


Prob-TransE      & 66.5       & 88.8      & 39.8      & 79.0      & 66.4       & 51.9      & 85.6      & 81.5    & 76.6  \\
JointE+SATT     &82.7 & \textbf{96.2} &45.0 &80.7 &81.7& 57.7 & 93.6 &84.0 & 79.3  \\ 

\midrule

Prob-TransD		&79.1	&93.0 &42.2	&79.2	&79.2	&51.6	&90.9	&82.7	&78.2  \\  

JointD+SATT    &\textbf{82.7}   &95.2 &\textbf{47.8} & \textbf{81.6} &\textbf{82.0} &\textbf{57.9} & \textbf{94.7} &\textbf{84.7} & \textbf{80.4}  \\ 
\bottomrule
\end{tabular}}
\caption{Evaluation results on link prediction of head and tail entities (\%).}
\label{t:entity}
\end{table*}



(1) All the sentence encoders combined with different multi-instance learning methods get significant improvements after trained under our joint learning framework. From the average results of the prediction accuracy, CNN+ONE increases by $6\%$ and CNN+ATT increases by $5\%$ after jointly learning with KGs. 

(2) As compared with the sentence encoders learned jointly with Prob-TransE, the encoders learned jointly with Prob-TransD further enhance the effect. Prob-TransD is more complex than Prob-TransE, which can better extract features of KGs and comprehend relationships between entities. The results demonstrate that the joint learning framework successfully takes better advantages of KGs to train CNN, and the effectiveness of KG models can affect the results. 

(3) In Table \ref{t:relationExt}, ATT and KATT perform much better than ONE. The training sentences constructed via distant supervision contain noises, in which not all sentences contain entity pairs can exactly indicate the textual relations. Hence, the attention mechanism is beneficial and effectively highlights the most meaningful sentences. 

(4) The comparison between ATT and KATT further shows that the simple attention mechanism without using information in KGs is not enough. The same relation often has nuances when it is between different entity pairs, and a vague global attention can not select important sentences accurately. Hence, the efficient information from KGs helps knowledge-based attention be more discriminative than sentence-level attention. This indicates the effectiveness of our knowledge-based attention mechanism.





\subsection{Results of Knowledge Graph Completion}

Entity link prediction has been used for KGC evaluation in \cite{bordes2013translating}. We need to predict the tail entity when given a triple $(h, r, ?)$ or predict the head entity when given a triple $(?, r ,t)$. We want to investigate the effectiveness of our joint model for improving KG models via this task.

\subsubsection{Evaluation Results}

For each test triple $(h, r, t)$, we replace head and tail entities with all entities in FB15K ranked in descending order of distance scores calculated by Eq. (\ref{eq:kg_distance}). The relational fact $(h, r, t)$ is expected to have a better score than any other corrupted triples. We follow previous works and use the proportion of correct entities in top-10 ranked entities (Hits@10) as the evaluation metric. 

The relations in KGs can be divided into four classes: 1-to-1, 1-to-N, N-to-1 and N-to-N relations \cite{bordes2013translating}. We report the average Hits@10 scores when predicting missing head entities and tail entities with respect to different classes of relations. We also report the overall performance by averaging the Hits@10 scores over triples. 

Since the evaluation setting is identical, we simply report the results of SE, SME, TransE, TransH, TransR/CTransR, TransD from \cite{bordes2011learning,bordes2012joint,bordes2013translating,wang2014transh,lin2015learning,ji2015knowledge}. The models for knowledge representation without joint learning in our framework are named as ``Prob-TransE'' and ``Prob-TransD''. The models learned under our joint learning framework with our semantics-based attention are named as ``JointE+SATT'' and ``JointD+SATT''. The results are shown in Table \ref{t:entity}. From the results, we observe that: 

% To further evaluate the effectiveness of our joint models, we compare the overall performance by averaging the Hits@10 scores over relations. The overall performance of Prob-TransE and JointE+SATT is $\textbf{66.2\%}$ and $\textbf{80.4\%}$ respectively. The overall performance of Prob-TransD and JointD+SATT is $\textbf{71.4\%}$ and $\textbf{85.1\%}$ respectively. 



(1) The joint models almost achieve improvements under four classes of relations when predicting head and tail entities. This indicates models trained under our joint framework can take advantages of plain texts and significantly improve representation power in relation-level. 

(2) The improvements on ``1-to-1'', ``1-to-N'' and ``N-to-1'' relations are much more significant as compared to those on ``N-to-N''. This indicates that our joint framework is more effective to embed textual relations for those deterministic relations. 

% In FB15K, more than $80\%$ triples are instances of ``N-to-N'' relations. Since the improvements of the joint models on ``N-to-N'' relations are not as remarkable as on other relation classes, the overall superiority of our joint models seems not so notable when averaging over triples. 

(3) TransD is a model extended from TransE and has a complicated entity embedding mechanism. After integrated into joint learning framework, it further improves its performance. It means that other knowledge graph representation methods similar to TransE and TransD, such as TransH and TransR, can also be integrated with the same way into our framework.



\begin{table}[]
\centering
\scalebox{0.9}{
\begin{tabular}{c|c|c}
\toprule
\textbf{Dataset}           & \textbf{Method}                   & \textbf{Hits@10}      \\
\midrule
\multirow{5}{*}{FB15K}     & DKRL\cite{xie2016representation}  & 67.4                  \\
                           & TEKE\cite{wang2016text}           & 73.0                  \\
                           & DESP\cite{zhong2015aligning}      & 77.3                  \\
                           & JointE+SATT                       & 79.3                  \\
                           & JointD+SATT                       & \textbf{80.4}         \\
\midrule
\multirow{6}{*}{FB15K-237} & E+DISTMULT       & \multirow{2}{*}{60.2} 					\\
                           &  \cite{toutanova2015representing}    &                    \\
                           & E+DISTMULT(CONV) & \multirow{2}{*}{61.1} 					\\
                           &  \cite{toutanova2015representing}    &                    \\
                           & JointE+SATT                       & 69.2                  \\
                           & JointD+SATT                       & \textbf{69.9}         \\
\bottomrule
\end{tabular}}
\caption{Evaluation results on link prediction of different joint learning models (\%).}
\label{t:compwithjointmodels}
\end{table}

\subsubsection{Comparison with Other Joint Learning Models}

We also compare our models with other joint learning models for KGC. DESP \cite{zhong2015aligning}, TEKE \cite{wang2016text}, and DKRL \cite{xie2016representation} learn entity embeddings from the knowledge graph and text descriptions. E+DISTMULT(CONV) \cite{toutanova2015representing} extracts textual relations using dependency parsing to incorporate text into DISTMULT. Following the previous experiment settings, we compare with DESP, TEKE, and DKRL in FB15K. We also use FB15K-237 aligned with our NYT corpus to train our joint models for comparison with E+DISTMULT(CONV).

The Evaluation results are shown in Table \ref{t:compwithjointmodels}. From the results, we observe that our joint models which directly encode from sentences outperform methods based on dependency parsing. Though we train our joint models with non-strictly aligned text corpus, our models are still significantly effective, even as compared with methods use strictly aligned text descriptions.

\section{Conclusion and Future Work}

In this paper, we propose a soft joint framework for representation learning of KGs and text, which is in view of advantages of the previous methods. Our framework embeds entities, relations, and words within a unified space. More specifically, the framework can work well with non-strictly aligned data. We also propose the alternating forward attention, which is made up of the knowledge-based attention and the semantics-based attention. These two parts alternately reduce noise and enhance joint models during the training process. On both RE and KGC, experiment results show that the joint learning framework effectively performs representation learning for both the KG and text. By incorporating different knowledge representation learning models, we also show the framework is open to existing models. In the future, we will explore to adopt RNN or LSTM for encoding textual relations in an efficient manner. To take more rich information especially some human-designed features as the guidance for our joint framework is also necessary.



% , and obtain more discriminative embeddings for knowledge acquisition. 




\bibliography{aaai2018}
\bibliographystyle{aaai}
\end{document}
